{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 고흐풍으로 스타일 바꾸기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "원 소스 위치 : https://github.com/log0/neural-style-painting/blob/master/art.py\n",
    "        \n",
    "수정된 사항\n",
    "- 미리학습된 파일을 다운로드 받도록 코드 추가\n",
    "- 대상 이미지의 위치를 images/에서 ./로 변경\n",
    "- 이미지 출력을 위해 matplolib 설정\n",
    "        \n",
    "        \n",
    "다음 2 파일을 다운로드\n",
    "- https://github.com/log0/neural-style-painting/blob/master/images/Macau.jpg\n",
    "- https://github.com/log0/neural-style-painting/blob/master/images/StarryNight.jpg\n",
    "\n",
    "Data > Upload… 메뉴로 업로드\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of the \"A Neural Algorithm of Artistic Style\" in Python using\n",
    "TensorFlow.\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Constants for the image input and output.\n",
    "###############################################################################\n",
    "\n",
    "# Output folder for the images.\n",
    "OUTPUT_DIR = 'output/'\n",
    "# Style image to use.\n",
    "# STYLE_IMAGE = 'images/starry_night.jpg'\n",
    "STYLE_IMAGE = '../StarryNight.jpg'\n",
    "# Content image to use.\n",
    "# CONTENT_IMAGE = 'images/hong_kong_2.jpg'\n",
    "CONTENT_IMAGE = '../Macau.jpg'\n",
    "# Image dimensions constants. \n",
    "IMAGE_WIDTH = 800\n",
    "IMAGE_HEIGHT = 600\n",
    "COLOR_CHANNELS = 3\n",
    "\n",
    "###############################################################################\n",
    "# Algorithm constants\n",
    "###############################################################################\n",
    "# Noise ratio. Percentage of weight of the noise for intermixing with the\n",
    "# content image.\n",
    "NOISE_RATIO = 0.6\n",
    "# Number of iterations to run.\n",
    "ITERATIONS = 5000\n",
    "# Constant to put more emphasis on content loss.\n",
    "BETA = 5\n",
    "# Constant to put more emphasis on style loss.\n",
    "ALPHA = 100\n",
    "# Path to the deep learning model. This is more than 500MB so will not be\n",
    "# included in the repository, but available to download at the model Zoo:\n",
    "# Link: https://github.com/BVLC/caffe/wiki/Model-Zoo\n",
    "#\n",
    "# Pick the VGG 19-layer model by from the paper \"Very Deep Convolutional \n",
    "# Networks for Large-Scale Image Recognition\".\n",
    "VGG_MODEL = 'imagenet-vgg-verydeep-19.mat'\n",
    "# The mean to subtract from the input to the VGG model. This is the mean that\n",
    "# when the VGG was used to train. Minor changes to this will make a lot of\n",
    "# difference to the performance of model.\n",
    "MEAN_VALUES = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))\n",
    "\n",
    "def show_image(image):\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "def generate_noise_image(content_image, noise_ratio = NOISE_RATIO):\n",
    "    \"\"\"\n",
    "    Returns a noise image intermixed with the content image at a certain ratio.\n",
    "    \"\"\"\n",
    "    noise_image = np.random.uniform(\n",
    "            -20, 20,\n",
    "            (1, IMAGE_HEIGHT, IMAGE_WIDTH, COLOR_CHANNELS)).astype('float32')\n",
    "    # White noise image from the content representation. Take a weighted average\n",
    "    # of the values\n",
    "    input_image = noise_image * noise_ratio + content_image * (1 - noise_ratio)\n",
    "    return input_image\n",
    "\n",
    "def load_image(path):\n",
    "    image = scipy.misc.imread(path)\n",
    "    show_image(image)\n",
    "    # Resize the image for convnet input, there is no change but just\n",
    "    # add an extra dimension.\n",
    "    image = np.reshape(image, ((1,) + image.shape))\n",
    "    # Input to the VGG model expects the mean to be subtracted.\n",
    "    image = image - MEAN_VALUES\n",
    "    return image\n",
    "\n",
    "def save_image(path, image):\n",
    "    # Output should add back the mean.\n",
    "    image = image + MEAN_VALUES\n",
    "    # Get rid of the first useless dimension, what remains is the image.\n",
    "    image = image[0]\n",
    "    image = np.clip(image, 0, 255).astype('uint8')\n",
    "    show_image(image)\n",
    "    scipy.misc.imsave(path, image)\n",
    "\n",
    "def load_vgg_model(path):\n",
    "    \"\"\"\n",
    "    Returns a model for the purpose of 'painting' the picture.\n",
    "\n",
    "    Takes only the convolution layer weights and wrap using the TensorFlow\n",
    "    Conv2d, Relu and AveragePooling layer. VGG actually uses maxpool but\n",
    "    the paper indicates that using AveragePooling yields better results.\n",
    "    The last few fully connected layers are not used.\n",
    "\n",
    "    Here is the detailed configuration of the VGG model:\n",
    "\n",
    "        0 is conv1_1 (3, 3, 3, 64)\n",
    "        1 is relu\n",
    "        2 is conv1_2 (3, 3, 64, 64)\n",
    "        3 is relu    \n",
    "        4 is maxpool\n",
    "        5 is conv2_1 (3, 3, 64, 128)\n",
    "        6 is relu\n",
    "        7 is conv2_2 (3, 3, 128, 128)\n",
    "        8 is relu\n",
    "        9 is maxpool\n",
    "        10 is conv3_1 (3, 3, 128, 256)\n",
    "        11 is relu\n",
    "        12 is conv3_2 (3, 3, 256, 256)\n",
    "        13 is relu\n",
    "        14 is conv3_3 (3, 3, 256, 256)\n",
    "        15 is relu\n",
    "        16 is conv3_4 (3, 3, 256, 256)\n",
    "        17 is relu\n",
    "        18 is maxpool\n",
    "        19 is conv4_1 (3, 3, 256, 512)\n",
    "        20 is relu\n",
    "        21 is conv4_2 (3, 3, 512, 512)\n",
    "        22 is relu\n",
    "        23 is conv4_3 (3, 3, 512, 512)\n",
    "        24 is relu\n",
    "        25 is conv4_4 (3, 3, 512, 512)\n",
    "        26 is relu\n",
    "        27 is maxpool\n",
    "        28 is conv5_1 (3, 3, 512, 512)\n",
    "        29 is relu\n",
    "        30 is conv5_2 (3, 3, 512, 512)\n",
    "        31 is relu\n",
    "        32 is conv5_3 (3, 3, 512, 512)\n",
    "        33 is relu\n",
    "        34 is conv5_4 (3, 3, 512, 512)\n",
    "        35 is relu\n",
    "        36 is maxpool\n",
    "        37 is fullyconnected (7, 7, 512, 4096)\n",
    "        38 is relu\n",
    "        39 is fullyconnected (1, 1, 4096, 4096)\n",
    "        40 is relu\n",
    "        41 is fullyconnected (1, 1, 4096, 1000)\n",
    "        42 is softmax\n",
    "    \"\"\"\n",
    "    vgg = scipy.io.loadmat(path)\n",
    "\n",
    "    vgg_layers = vgg['layers']\n",
    "    def _weights(layer, expected_layer_name):\n",
    "        \"\"\"\n",
    "        Return the weights and bias from the VGG model for a given layer.\n",
    "        \"\"\"\n",
    "        W = vgg_layers[0][layer][0][0][0][0][0]\n",
    "        b = vgg_layers[0][layer][0][0][0][0][1]\n",
    "        layer_name = vgg_layers[0][layer][0][0][-2]\n",
    "        assert layer_name == expected_layer_name\n",
    "        return W, b\n",
    "\n",
    "    def _relu(conv2d_layer):\n",
    "        \"\"\"\n",
    "        Return the RELU function wrapped over a TensorFlow layer. Expects a\n",
    "        Conv2d layer input.\n",
    "        \"\"\"\n",
    "        return tf.nn.relu(conv2d_layer)\n",
    "\n",
    "    def _conv2d(prev_layer, layer, layer_name):\n",
    "        \"\"\"\n",
    "        Return the Conv2D layer using the weights, biases from the VGG\n",
    "        model at 'layer'.\n",
    "        \"\"\"\n",
    "        W, b = _weights(layer, layer_name)\n",
    "        W = tf.constant(W)\n",
    "        b = tf.constant(np.reshape(b, (b.size)))\n",
    "        return tf.nn.conv2d(\n",
    "            prev_layer, filter=W, strides=[1, 1, 1, 1], padding='SAME') + b\n",
    "\n",
    "    def _conv2d_relu(prev_layer, layer, layer_name):\n",
    "        \"\"\"\n",
    "        Return the Conv2D + RELU layer using the weights, biases from the VGG\n",
    "        model at 'layer'.\n",
    "        \"\"\"\n",
    "        return _relu(_conv2d(prev_layer, layer, layer_name))\n",
    "\n",
    "    def _avgpool(prev_layer):\n",
    "        \"\"\"\n",
    "        Return the AveragePooling layer.\n",
    "        \"\"\"\n",
    "        return tf.nn.avg_pool(prev_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Constructs the graph model.\n",
    "    graph = {}\n",
    "    graph['input']   = tf.Variable(np.zeros((1, IMAGE_HEIGHT, IMAGE_WIDTH, COLOR_CHANNELS)), dtype = 'float32')\n",
    "    graph['conv1_1']  = _conv2d_relu(graph['input'], 0, 'conv1_1')\n",
    "    graph['conv1_2']  = _conv2d_relu(graph['conv1_1'], 2, 'conv1_2')\n",
    "    graph['avgpool1'] = _avgpool(graph['conv1_2'])\n",
    "    graph['conv2_1']  = _conv2d_relu(graph['avgpool1'], 5, 'conv2_1')\n",
    "    graph['conv2_2']  = _conv2d_relu(graph['conv2_1'], 7, 'conv2_2')\n",
    "    graph['avgpool2'] = _avgpool(graph['conv2_2'])\n",
    "    graph['conv3_1']  = _conv2d_relu(graph['avgpool2'], 10, 'conv3_1')\n",
    "    graph['conv3_2']  = _conv2d_relu(graph['conv3_1'], 12, 'conv3_2')\n",
    "    graph['conv3_3']  = _conv2d_relu(graph['conv3_2'], 14, 'conv3_3')\n",
    "    graph['conv3_4']  = _conv2d_relu(graph['conv3_3'], 16, 'conv3_4')\n",
    "    graph['avgpool3'] = _avgpool(graph['conv3_4'])\n",
    "    graph['conv4_1']  = _conv2d_relu(graph['avgpool3'], 19, 'conv4_1')\n",
    "    graph['conv4_2']  = _conv2d_relu(graph['conv4_1'], 21, 'conv4_2')\n",
    "    graph['conv4_3']  = _conv2d_relu(graph['conv4_2'], 23, 'conv4_3')\n",
    "    graph['conv4_4']  = _conv2d_relu(graph['conv4_3'], 25, 'conv4_4')\n",
    "    graph['avgpool4'] = _avgpool(graph['conv4_4'])\n",
    "    graph['conv5_1']  = _conv2d_relu(graph['avgpool4'], 28, 'conv5_1')\n",
    "    graph['conv5_2']  = _conv2d_relu(graph['conv5_1'], 30, 'conv5_2')\n",
    "    graph['conv5_3']  = _conv2d_relu(graph['conv5_2'], 32, 'conv5_3')\n",
    "    graph['conv5_4']  = _conv2d_relu(graph['conv5_3'], 34, 'conv5_4')\n",
    "    graph['avgpool5'] = _avgpool(graph['conv5_4'])\n",
    "    return graph\n",
    "\n",
    "def content_loss_func(sess, model):\n",
    "    \"\"\"\n",
    "    Content loss function as defined in the paper.\n",
    "    \"\"\"\n",
    "    def _content_loss(p, x):\n",
    "        # N is the number of filters (at layer l).\n",
    "        N = p.shape[3]\n",
    "        # M is the height times the width of the feature map (at layer l).\n",
    "        M = p.shape[1] * p.shape[2]\n",
    "        # Interestingly, the paper uses this form instead:\n",
    "        #\n",
    "        #   0.5 * tf.reduce_sum(tf.pow(x - p, 2)) \n",
    "        #\n",
    "        # But this form is very slow in \"painting\" and thus could be missing\n",
    "        # out some constants (from what I see in other source code), so I'll\n",
    "        # replicate the same normalization constant as used in style loss.\n",
    "        return (1 / (4 * N * M)) * tf.reduce_sum(tf.pow(x - p, 2))\n",
    "    return _content_loss(sess.run(model['conv4_2']), model['conv4_2'])\n",
    "\n",
    "def style_loss_func(sess, model):\n",
    "    \"\"\"\n",
    "    Style loss function as defined in the paper.\n",
    "    \"\"\"\n",
    "    def _gram_matrix(F, N, M):\n",
    "        \"\"\"\n",
    "        The gram matrix G.\n",
    "        \"\"\"\n",
    "        Ft = tf.reshape(F, (M, N))\n",
    "        return tf.matmul(tf.transpose(Ft), Ft)\n",
    "\n",
    "    def _style_loss(a, x):\n",
    "        \"\"\"\n",
    "        The style loss calculation.\n",
    "        \"\"\"\n",
    "        # N is the number of filters (at layer l).\n",
    "        N = a.shape[3]\n",
    "        # M is the height times the width of the feature map (at layer l).\n",
    "        M = a.shape[1] * a.shape[2]\n",
    "        # A is the style representation of the original image (at layer l).\n",
    "        A = _gram_matrix(a, N, M)\n",
    "        # G is the style representation of the generated image (at layer l).\n",
    "        G = _gram_matrix(x, N, M)\n",
    "        result = (1 / (4 * N**2 * M**2)) * tf.reduce_sum(tf.pow(G - A, 2))\n",
    "        return result\n",
    "\n",
    "    # Layers to use. We will use these layers as advised in the paper.\n",
    "    # To have softer features, increase the weight of the higher layers\n",
    "    # (conv5_1) and decrease the weight of the lower layers (conv1_1).\n",
    "    # To have harder features, decrease the weight of the higher layers\n",
    "    # (conv5_1) and increase the weight of the lower layers (conv1_1).\n",
    "    layers = [\n",
    "        ('conv1_1', 0.5),\n",
    "        ('conv2_1', 1.0),\n",
    "        ('conv3_1', 1.5),\n",
    "        ('conv4_1', 3.0),\n",
    "        ('conv5_1', 4.0),\n",
    "    ]\n",
    "\n",
    "    E = [_style_loss(sess.run(model[layer_name]), model[layer_name]) for layer_name, _ in layers]\n",
    "    W = [w for _, w in layers]\n",
    "    loss = sum([W[l] * E[l] for l in range(len(layers))])\n",
    "    return loss\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    if not os.path.exists(VGG_MODEL):\n",
    "        print(\"downloading pretrained model... it's 570M. be patient.\")\n",
    "        # 원 파일 위치 : \"https://drive.google.com/file/d/0B8QJdgMvQDrVU2cyZjFKU1RrLUU/view?usp=sharing\"\n",
    "        urllib.request.urlretrieve(\"https://www.dropbox.com/s/q7jwj6fadtofgkx/imagenet-vgg-verydeep-19.mat?dl=1\", VGG_MODEL)\n",
    "        print(\"download complete\")\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Load the images.\n",
    "        content_image = load_image(CONTENT_IMAGE)\n",
    "        style_image = load_image(STYLE_IMAGE)\n",
    "\n",
    "            \n",
    "        # Load the model.\n",
    "        model = load_vgg_model(VGG_MODEL)\n",
    "\n",
    "        # Generate the white noise and content presentation mixed image\n",
    "        # which will be the basis for the algorithm to \"paint\".\n",
    "        input_image = generate_noise_image(content_image)\n",
    "\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        # Construct content_loss using content_image.\n",
    "        sess.run(model['input'].assign(content_image))\n",
    "        content_loss = content_loss_func(sess, model)\n",
    "\n",
    "        # Construct style_loss using style_image.\n",
    "        sess.run(model['input'].assign(style_image))\n",
    "        style_loss = style_loss_func(sess, model)\n",
    "\n",
    "        # Instantiate equation 7 of the paper.\n",
    "        total_loss = BETA * content_loss + ALPHA * style_loss\n",
    "\n",
    "        # From the paper: jointly minimize the distance of a white noise image\n",
    "        # from the content representation of the photograph in one layer of\n",
    "        # the neywork and the style representation of the painting in a number\n",
    "        # of layers of the CNN.\n",
    "        #\n",
    "        # The content is built from one layer, while the style is from five\n",
    "        # layers. Then we minimize the total_loss, which is the equation 7.\n",
    "        optimizer = tf.train.AdamOptimizer(2.0)\n",
    "        train_step = optimizer.minimize(total_loss)\n",
    "\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        sess.run(model['input'].assign(input_image))\n",
    "        for it in range(ITERATIONS):\n",
    "            sess.run(train_step)\n",
    "\n",
    "            if it%10 == 0:\n",
    "                mixed_image = sess.run(model['input'])\n",
    "                print('Iteration %d' % (it))\n",
    "                print('sum : ', sess.run(tf.reduce_sum(mixed_image)))\n",
    "                print('cost: ', sess.run(total_loss))\n",
    "\n",
    "                if not os.path.exists(OUTPUT_DIR):\n",
    "                    os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "                filename = 'output/%d.png' % (it)\n",
    "                save_image(filename, mixed_image)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
